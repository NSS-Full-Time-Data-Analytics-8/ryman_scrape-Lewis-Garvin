{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a907d2fb",
   "metadata": {},
   "source": [
    "## Web Scraping the Ryman Calendar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b21272",
   "metadata": {},
   "source": [
    "In this exercise, your objective is to use BeautifulSoup in order to obtain a dataset of upcoming events at the Ryman. This information is available at https://ryman.com/events/, but you will take the contents of this website and convert it into a pandas DataFrame.\n",
    "\n",
    "The website splits the events across multiple pages, but start by just working on the first page. Later on in the exercise, you'll take what you've done for the first page and apply it across other pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aeb9a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from bs4 import Comment\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d27754a",
   "metadata": {},
   "source": [
    "1. Start by using either the inspector or by viewing the page source. Can you identify a tag that might be helpful for finding the names of all performers? For now, just worry about the headliner and don't worry about the opener. (Eg. For Vince Gill, featuring Wendy Moten, we only care about Vince Gill.) Make use of this to create a list containing just the names of each inductee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c382abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://ryman.com/events/'\n",
    "response = requests.get(URL)\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343d71a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BS(response.text)\n",
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a1d9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tribe_events = soup.findAll('a', attrs = {'class':'tribe-event-url'})\n",
    "performers = [x.get('title') for x in tribe_events]\n",
    "print(len(performers))\n",
    "#performers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fccee8",
   "metadata": {},
   "source": [
    "2. Next, try and find a tag that could be used to find the date and time for each show. Extract these into two lists, one containing the date and the other containing the time. (Eg. THURSDAY, AUGUST 4, 2022 AT 8:00 PM CDT should be split into August 4, 2022 and 8:00 PM CDT.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cab91b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "event_time_tags = soup.findAll('time')\n",
    "event_datetimes = [x.text for x in event_time_tags]\n",
    "\n",
    "print(len(event_datetimes))\n",
    "#event_datetimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5814d066",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_datetimes = [x.split(' at ') for x in event_datetimes]\n",
    "event_dates_full = [x[0] for x in event_datetimes]\n",
    "#Using regex is probably overkill here, but including it here for practice.\n",
    "event_dates = [re.search('[a-zA-Z]+,\\s(.+)', x).group(1) for x in event_dates_full]\n",
    "event_times = [x[1] for x in event_datetimes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d38514c",
   "metadata": {},
   "source": [
    "3. Take the two lists you created on parts 1 and 2 and convert it into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc6a57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'performer':performers,'event_date':event_dates, 'event_time':event_times}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86691c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df = pd.DataFrame(d)\n",
    "print(len(events_df))\n",
    "events_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0399247f",
   "metadata": {},
   "source": [
    "4. Now, you need to take what you created for the first page and apply it across multiple rest of the pages so that you can scrape all inductees. Notice how the url changes when you click the \"More Events\" button at the top of the page. Check that the code that you wrote for the first page still works for page 2. Once you have verified that your code will still work, write a for loop that will cycle through the first five pages of events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1855c469",
   "metadata": {},
   "outputs": [],
   "source": [
    "performers = []\n",
    "event_dates = []\n",
    "event_times = []\n",
    "\n",
    "for x in range(5):\n",
    "    URL='https://ryman.com/events/list/?tribe_event_display=list&tribe_paged=' + str(x+1)\n",
    "    \n",
    "    response = requests.get(URL)\n",
    "    if response.status_code == 200:\n",
    "        soup = BS(response.text)\n",
    "\n",
    "        #Note: This approach finds all of each type of tag separately. \n",
    "        #      The approach used in Bonus #1 finds all for a higher parent tag and then finds tags within that,\n",
    "        #      which I think is the better of the two.\n",
    "        tribe_events = soup.findAll('a', attrs = {'class':'tribe-event-url'})\n",
    "        page_performers = [x.get('title') for x in tribe_events]\n",
    "        performers = performers + page_performers\n",
    "\n",
    "        event_time_tags = soup.findAll('time')\n",
    "        event_datetimes = [x.text for x in event_time_tags]\n",
    "        event_datetimes_split = [x.split(' at ') for x in event_datetimes]\n",
    "        \n",
    "        page_event_dates_full = [x[0] for x in event_datetimes_split]\n",
    "        page_event_dates = [re.search('[a-zA-Z]+,\\s(.+)', x).group(1) for x in page_event_dates_full]\n",
    "        page_event_times = [x[1] for x in event_datetimes_split]\n",
    "\n",
    "        event_dates = event_dates + page_event_dates\n",
    "        event_times = event_times + page_event_times\n",
    "\n",
    "d = {'performer':performers,'event_date':event_dates, 'event_time':event_times}\n",
    "events_df = pd.DataFrame(d)\n",
    "\n",
    "events_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c52a97a",
   "metadata": {},
   "source": [
    "5. **Bonus #1:**: Add to your data frame the opening act for all shows that list an opener."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c448bf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#WORKING VERSION -- LOOK AT PRIOR COMMENTS FOR OPENERS\n",
    "performers = []\n",
    "openers = []\n",
    "event_dates = []\n",
    "event_times = []\n",
    "event_hrefs = []   #included to help set up Bonus #2\n",
    "event_prices = []  #included to help set up Bonus #2\n",
    "\n",
    "for x in range(5):\n",
    "    URL='https://ryman.com/events/list/?tribe_event_display=list&tribe_paged=' + str(x+1)\n",
    "    \n",
    "    response = requests.get(URL)\n",
    "    if response.status_code == 200:\n",
    "        soup = BS(response.text)\n",
    "        \n",
    "        tribe_beside_image_tags = soup.findAll('div', attrs={'class':'tribe-beside-image'})\n",
    "        \n",
    "        for tribe_beside_image_tag in tribe_beside_image_tags:\n",
    "            \n",
    "            #find tribe_event_url_tag\n",
    "            tribe_event_url_tag = tribe_beside_image_tag.find('a', attrs={'class':'tribe-event-url'})\n",
    "            \n",
    "            #get performer from tribe_event_url_tag and add to list of performers\n",
    "            performer = tribe_event_url_tag.get('title')\n",
    "            performers = performers + [performer]\n",
    "\n",
    "            #get href from tribe_event_url_tag and add to list of hrefs\n",
    "            event_href = tribe_event_url_tag.get('href')\n",
    "            event_hrefs = event_hrefs + [event_href]\n",
    "\n",
    "            #ISSUE: CAN BE MULTIPLE OPENER TAGS\n",
    "            #ISSUE: HOW TO DETERMINE IF OPENER TAG IS ACTUALLY AN OPENER\n",
    "            #find opener tag, get text, and add to openers list\n",
    "            \n",
    "            #Set opener to None as its default value. Opener will be changed if a legitimate opener is found.\n",
    "            opener = None\n",
    "            \n",
    "            #Find all opener tags\n",
    "            opener_tags = tribe_beside_image_tag.findAll('span', attrs={'class':'opener'})\n",
    "            \n",
    "            #if no opener tags are found, for loop will end immediately.\n",
    "            for opener_tag in opener_tags:\n",
    "                #the opener tag must have an 'OPENER' comment preceding it.\n",
    "                if isinstance(opener_tag.previous_sibling.previous_sibling, Comment):\n",
    "                    if str(opener_tag.previous_sibling.previous_sibling).strip().upper() == 'OPENER':\n",
    "                        #the opener text must start with 'With ', 'Featuring', or 'Hosted by'.\n",
    "                        if opener_tag.text.upper()[:5] == 'WITH ':\n",
    "                            opener = opener_tag.text\n",
    "                        elif opener_tag.text.upper()[:9] == 'FEATURING':\n",
    "                            opener = opener_tag.text\n",
    "                        elif opener_tag.text.upper()[:9] == 'HOSTED BY':\n",
    "                            opener = opener_tag.text\n",
    "\n",
    "            openers = openers + [opener]\n",
    "\n",
    "            #find datetime tag, get text, and split out date and time\n",
    "            datetime_tag = tribe_beside_image_tag.find('time')\n",
    "            datetime = datetime_tag.text.split(' at ')\n",
    "            event_date = re.search('[a-zA-Z]+,\\s(.+)', datetime[0]).group(1)\n",
    "            event_dates = event_dates + [event_date]\n",
    "            event_times = event_times + [datetime[1]]\n",
    "\n",
    "            #Build list of empty values (must be same length as other lists)\n",
    "            event_prices = event_prices + [None]\n",
    "            \n",
    "event_dict = {'performer':performers,\n",
    "              'opener':openers,\n",
    "              'event_date':event_dates, \n",
    "              'event_time':event_times,\n",
    "              'event_href':event_hrefs,\n",
    "              'event_price':event_prices}\n",
    "events_df = pd.DataFrame(event_dict)\n",
    "events_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06d12ad",
   "metadata": {},
   "source": [
    "5. **Bonus #2:** If you click the \"MORE INFO\" button for an event, it will take you to a page which shows ticket prices. Write code that can be used to retrieve the ticket prices for each show that you have scraped. Make sure that your code can handle cases where the show has been canceled (eg. https://ryman.com/event/nhabit-worship-experience/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260eac11",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(events_df.shape[0]):\n",
    "    url=events_df.event_href[x]\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BS(response.text)\n",
    "        event_price_tag = soup.find('p', attrs={'class':'theprices'})\n",
    "        if event_price_tag is not None:\n",
    "            event_price = str(event_price_tag.text)\n",
    "            \n",
    "            #NOT SURE WHETHER TO INCLUDE THIS:\n",
    "            #Add escape character before $ so it will display appropriately.\n",
    "            event_price = event_price.replace('$', '\\$')\n",
    "            \n",
    "            events_df.event_price[x]=event_price\n",
    "            #print(events_df.event_price[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b03819f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "events_df = events_df[['performer',\n",
    "                       'opener',\n",
    "                       'event_date',\n",
    "                       'event_time',\n",
    "                       'event_price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e84cfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1af27d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df.to_csv('../data/ryman_events_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73213197",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "events_df.event_price.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
